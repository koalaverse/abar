<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Advanced Business Analytics with R: Descriptive, Predictive, and Prescriptive Methods</title>
  <meta name="description" content="A deep-dive into using R for descriptive, predictive, and prescriptive analytics.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Advanced Business Analytics with R: Descriptive, Predictive, and Prescriptive Methods" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A deep-dive into using R for descriptive, predictive, and prescriptive analytics." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Advanced Business Analytics with R: Descriptive, Predictive, and Prescriptive Methods" />
  
  <meta name="twitter:description" content="A deep-dive into using R for descriptive, predictive, and prescriptive analytics." />
  

<meta name="author" content="Bradley C. Boehmke and Brandon M. Greenwell">


<meta name="date" content="2018-10-21">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="intro.html">
<link rel="next" href="visualization.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="extra.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Advanced Business Analytics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-this-book-is-for"><i class="fa fa-check"></i>Who this book is for</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-this-book-is-not-for"><i class="fa fa-check"></i>Who this book is not for</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-this-book-is-really-not-for"><i class="fa fa-check"></i>Who this book is really not for</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-r"><i class="fa fa-check"></i>Why R</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-this-book-is-organized"><i class="fa fa-check"></i>How this book is organized</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#conventions-used-in-this-book"><i class="fa fa-check"></i>Conventions used in this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#using-code-examples"><i class="fa fa-check"></i>Using code examples</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-information"><i class="fa fa-check"></i>Software information</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="author.html"><a href="author.html"><i class="fa fa-check"></i>Who are these Guys?</a><ul>
<li class="chapter" data-level="" data-path="author.html"><a href="author.html#bradley-c.-boehmke"><i class="fa fa-check"></i>Bradley C. Boehmke</a></li>
<li class="chapter" data-level="" data-path="author.html"><a href="author.html#brandon-m.-greenwell"><i class="fa fa-check"></i>Brandon M. Greenwell</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#descriptive"><i class="fa fa-check"></i><b>1.1</b> Descriptive</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#predictive"><i class="fa fa-check"></i><b>1.2</b> Predictive</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#prescriptive"><i class="fa fa-check"></i><b>1.3</b> Prescriptive</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#data"><i class="fa fa-check"></i><b>1.4</b> Data sets</a><ul>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#ames-iowa-housing-data"><i class="fa fa-check"></i>Ames Iowa housing data</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#employee-attrition-data"><i class="fa fa-check"></i>Employee attrition data</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>I Descriptive Analytics</b></span></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html#descriptive"><i class="fa fa-check"></i><b>2</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="descriptive.html"><a href="descriptive.html"><i class="fa fa-check"></i><b>2.1</b> Prerequisites</a></li>
<li class="chapter" data-level="2.2" data-path="descriptive.html"><a href="descriptive.html#measures-of-location"><i class="fa fa-check"></i><b>2.2</b> Measures of location</a><ul>
<li class="chapter" data-level="2.2.1" data-path="descriptive.html"><a href="descriptive.html#the-sample-mean"><i class="fa fa-check"></i><b>2.2.1</b> The sample mean</a></li>
<li class="chapter" data-level="2.2.2" data-path="descriptive.html"><a href="descriptive.html#the-sample-median"><i class="fa fa-check"></i><b>2.2.2</b> The sample median</a></li>
<li class="chapter" data-level="2.2.3" data-path="descriptive.html"><a href="descriptive.html#the-mean-or-the-median"><i class="fa fa-check"></i><b>2.2.3</b> The mean or the median</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="descriptive.html"><a href="descriptive.html#measures-of-spread"><i class="fa fa-check"></i><b>2.3</b> Measures of spread</a><ul>
<li class="chapter" data-level="2.3.1" data-path="descriptive.html"><a href="descriptive.html#empirical-rule"><i class="fa fa-check"></i><b>2.3.1</b> The empirical rule</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="descriptive.html"><a href="descriptive.html#percentiles"><i class="fa fa-check"></i><b>2.4</b> Percentiles</a></li>
<li class="chapter" data-level="2.5" data-path="descriptive.html"><a href="descriptive.html#robust-measures-of-spread"><i class="fa fa-check"></i><b>2.5</b> Robust measures of spread</a></li>
<li class="chapter" data-level="2.6" data-path="descriptive.html"><a href="descriptive.html#outliers"><i class="fa fa-check"></i><b>2.6</b> Outlier detection</a></li>
<li class="chapter" data-level="2.7" data-path="descriptive.html"><a href="descriptive.html#categorical"><i class="fa fa-check"></i><b>2.7</b> Describing categorical data</a><ul>
<li class="chapter" data-level="2.7.1" data-path="descriptive.html"><a href="descriptive.html#contingency-tables"><i class="fa fa-check"></i><b>2.7.1</b> Contingency tables</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="descriptive.html"><a href="descriptive.html#exercises"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="visualization.html"><a href="visualization.html"><i class="fa fa-check"></i><b>3</b> Visual data exploration</a><ul>
<li class="chapter" data-level="3.1" data-path="visualization.html"><a href="visualization.html#prerequisites-1"><i class="fa fa-check"></i><b>3.1</b> Prerequisites</a></li>
<li class="chapter" data-level="3.2" data-path="visualization.html"><a href="visualization.html#univariate-data"><i class="fa fa-check"></i><b>3.2</b> Univariate data</a><ul>
<li class="chapter" data-level="3.2.1" data-path="visualization.html"><a href="visualization.html#continuous-variables"><i class="fa fa-check"></i><b>3.2.1</b> Continuous Variables</a></li>
<li class="chapter" data-level="3.2.2" data-path="visualization.html"><a href="visualization.html#categorical-variables"><i class="fa fa-check"></i><b>3.2.2</b> Categorical Variables</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="visualization.html"><a href="visualization.html#bivariate-data"><i class="fa fa-check"></i><b>3.3</b> Bivariate data</a><ul>
<li class="chapter" data-level="3.3.1" data-path="visualization.html"><a href="visualization.html#scatter-plots"><i class="fa fa-check"></i><b>3.3.1</b> Scatter plots</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="visualization.html"><a href="visualization.html#multivariate-data"><i class="fa fa-check"></i><b>3.4</b> Multivariate data</a><ul>
<li class="chapter" data-level="3.4.1" data-path="visualization.html"><a href="visualization.html#facetting"><i class="fa fa-check"></i><b>3.4.1</b> Facetting</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="visualization.html"><a href="visualization.html#data-quality"><i class="fa fa-check"></i><b>3.5</b> Data quality</a></li>
<li class="chapter" data-level="3.6" data-path="visualization.html"><a href="visualization.html#further-reading"><i class="fa fa-check"></i><b>3.6</b> Further reading</a></li>
<li class="chapter" data-level="3.7" data-path="visualization.html"><a href="visualization.html#exercises-1"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>4</b> Statistical Inference</a><ul>
<li class="chapter" data-level="4.1" data-path="inference.html"><a href="inference.html#the-frequentist-approach"><i class="fa fa-check"></i><b>4.1</b> The frequentist approach</a><ul>
<li class="chapter" data-level="4.1.1" data-path="inference.html"><a href="inference.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>4.1.1</b> The central limit theorem</a></li>
<li class="chapter" data-level="4.1.2" data-path="inference.html"><a href="inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>4.1.2</b> Hypothesis testing</a></li>
<li class="chapter" data-level="4.1.3" data-path="inference.html"><a href="inference.html#one-sided-versus-two-sided-tests"><i class="fa fa-check"></i><b>4.1.3</b> One-sided versus two-sided tests</a></li>
<li class="chapter" data-level="4.1.4" data-path="inference.html"><a href="inference.html#type-i-and-type-ii-errors"><i class="fa fa-check"></i><b>4.1.4</b> Type I and type II errors</a></li>
<li class="chapter" data-level="4.1.5" data-path="inference.html"><a href="inference.html#p-values"><i class="fa fa-check"></i><b>4.1.5</b> <span class="math inline">\(p\)</span>-values</a></li>
<li class="chapter" data-level="4.1.6" data-path="inference.html"><a href="inference.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1.6</b> Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="inference.html"><a href="inference.html#one--and-two-sample-t-tests"><i class="fa fa-check"></i><b>4.2</b> One- and two-sample t-tests</a><ul>
<li class="chapter" data-level="4.2.1" data-path="inference.html"><a href="inference.html#one-sample-t-test"><i class="fa fa-check"></i><b>4.2.1</b> One-sample t-test</a></li>
<li class="chapter" data-level="4.2.2" data-path="inference.html"><a href="inference.html#two-sample-t-test"><i class="fa fa-check"></i><b>4.2.2</b> Two-sample t-test</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="inference.html"><a href="inference.html#tests-involving-more-than-two-means-anova-models"><i class="fa fa-check"></i><b>4.3</b> Tests involving more than two means: ANOVA models</a></li>
<li class="chapter" data-level="4.4" data-path="inference.html"><a href="inference.html#testing-for-association-in-contingency-tables"><i class="fa fa-check"></i><b>4.4</b> Testing for association in contingency tables</a></li>
<li class="chapter" data-level="4.5" data-path="inference.html"><a href="inference.html#nonparametric-tests"><i class="fa fa-check"></i><b>4.5</b> Nonparametric tests</a></li>
<li class="chapter" data-level="4.6" data-path="inference.html"><a href="inference.html#bootstrap"><i class="fa fa-check"></i><b>4.6</b> The nonparametric bootstrap</a><ul>
<li class="chapter" data-level="4.6.1" data-path="inference.html"><a href="inference.html#bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>4.6.1</b> Bootstrap confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="inference.html"><a href="inference.html#further-reading-1"><i class="fa fa-check"></i><b>4.7</b> Further reading</a></li>
<li class="chapter" data-level="4.8" data-path="inference.html"><a href="inference.html#exercises-2"><i class="fa fa-check"></i><b>4.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="unsupervised.html"><a href="unsupervised.html"><i class="fa fa-check"></i><b>5</b> Unsupervised learning</a><ul>
<li class="chapter" data-level="5.1" data-path="unsupervised.html"><a href="unsupervised.html#prerequisites-2"><i class="fa fa-check"></i><b>5.1</b> Prerequisites</a></li>
<li class="chapter" data-level="5.2" data-path="unsupervised.html"><a href="unsupervised.html#pca"><i class="fa fa-check"></i><b>5.2</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="5.2.1" data-path="unsupervised.html"><a href="unsupervised.html#finding-principal-components"><i class="fa fa-check"></i><b>5.2.1</b> Finding principal components</a></li>
<li class="chapter" data-level="5.2.2" data-path="unsupervised.html"><a href="unsupervised.html#performing-pca-in-r"><i class="fa fa-check"></i><b>5.2.2</b> Performing PCA in R</a></li>
<li class="chapter" data-level="5.2.3" data-path="unsupervised.html"><a href="unsupervised.html#selecting-the-number-of-principal-components"><i class="fa fa-check"></i><b>5.2.3</b> Selecting the Number of Principal Components</a></li>
<li class="chapter" data-level="5.2.4" data-path="unsupervised.html"><a href="unsupervised.html#extracting-additional-insights"><i class="fa fa-check"></i><b>5.2.4</b> Extracting additional insights</a></li>
<li class="chapter" data-level="5.2.5" data-path="unsupervised.html"><a href="unsupervised.html#pca-with-mixed-data"><i class="fa fa-check"></i><b>5.2.5</b> PCA with mixed data</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="unsupervised.html"><a href="unsupervised.html#cluster-analysis"><i class="fa fa-check"></i><b>5.3</b> Cluster Analysis</a><ul>
<li class="chapter" data-level="5.3.1" data-path="unsupervised.html"><a href="unsupervised.html#clustering-distance-measures"><i class="fa fa-check"></i><b>5.3.1</b> Clustering distance measures</a></li>
<li class="chapter" data-level="5.3.2" data-path="unsupervised.html"><a href="unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>5.3.2</b> K-means clustering</a></li>
<li class="chapter" data-level="5.3.3" data-path="unsupervised.html"><a href="unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>5.3.3</b> Hierarchical clustering</a></li>
<li class="chapter" data-level="5.3.4" data-path="unsupervised.html"><a href="unsupervised.html#clustering-with-mixed-data"><i class="fa fa-check"></i><b>5.3.4</b> Clustering with mixed data</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Predictive Analytics</b></span><ul>
<li class="chapter" data-level="5.4" data-path="unsupervised.html"><a href="unsupervised.html#regression-problems"><i class="fa fa-check"></i><b>5.4</b> Regression problems</a></li>
<li class="chapter" data-level="5.5" data-path="unsupervised.html"><a href="unsupervised.html#classification-problems"><i class="fa fa-check"></i><b>5.5</b> Classification problems</a></li>
<li class="chapter" data-level="5.6" data-path="unsupervised.html"><a href="unsupervised.html#algorithm-comparison-guide"><i class="fa fa-check"></i><b>5.6</b> Algorithm Comparison Guide</a></li>
<li class="chapter" data-level="5.7" data-path="unsupervised.html"><a href="unsupervised.html#general-modeling-process"><i class="fa fa-check"></i><b>5.7</b> General modeling process</a><ul>
<li class="chapter" data-level="5.7.1" data-path="unsupervised.html"><a href="unsupervised.html#reg_perf_prereq"><i class="fa fa-check"></i><b>5.7.1</b> Prerequisites</a></li>
<li class="chapter" data-level="5.7.2" data-path="unsupervised.html"><a href="unsupervised.html#reg-perf-split"><i class="fa fa-check"></i><b>5.7.2</b> Data splitting</a></li>
<li class="chapter" data-level="5.7.3" data-path="unsupervised.html"><a href="unsupervised.html#reg_perf_feat"><i class="fa fa-check"></i><b>5.7.3</b> Feature engineering</a></li>
<li class="chapter" data-level="5.7.4" data-path="unsupervised.html"><a href="unsupervised.html#model-form"><i class="fa fa-check"></i><b>5.7.4</b> Basic model formulation</a></li>
<li class="chapter" data-level="5.7.5" data-path="unsupervised.html"><a href="unsupervised.html#tune"><i class="fa fa-check"></i><b>5.7.5</b> Model tuning</a></li>
<li class="chapter" data-level="5.7.6" data-path="unsupervised.html"><a href="unsupervised.html#cv"><i class="fa fa-check"></i><b>5.7.6</b> Cross Validation for Generalization</a></li>
<li class="chapter" data-level="5.7.7" data-path="unsupervised.html"><a href="unsupervised.html#reg-perf-eval"><i class="fa fa-check"></i><b>5.7.7</b> Model evaluation</a></li>
<li class="chapter" data-level="5.7.8" data-path="unsupervised.html"><a href="unsupervised.html#interpreting-predictive-models"><i class="fa fa-check"></i><b>5.7.8</b> Interpreting predictive models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>6</b> Linear regression</a><ul>
<li class="chapter" data-level="6.1" data-path="linear-regression.html"><a href="linear-regression.html#prerequisites-3"><i class="fa fa-check"></i><b>6.1</b> Prerequisites</a></li>
<li class="chapter" data-level="6.2" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.2</b> Simple linear regression</a></li>
<li class="chapter" data-level="6.3" data-path="linear-regression.html"><a href="linear-regression.html#multi-lm"><i class="fa fa-check"></i><b>6.3</b> Multiple linear regression</a></li>
<li class="chapter" data-level="6.4" data-path="linear-regression.html"><a href="linear-regression.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>6.4</b> Assessing Model Accuracy</a></li>
<li class="chapter" data-level="6.5" data-path="linear-regression.html"><a href="linear-regression.html#model-concerns"><i class="fa fa-check"></i><b>6.5</b> Model concerns</a></li>
<li class="chapter" data-level="6.6" data-path="linear-regression.html"><a href="linear-regression.html#principal-component-regression"><i class="fa fa-check"></i><b>6.6</b> Principal component regression</a></li>
<li class="chapter" data-level="6.7" data-path="linear-regression.html"><a href="linear-regression.html#partial-least-squares"><i class="fa fa-check"></i><b>6.7</b> Partial least squares</a></li>
<li class="chapter" data-level="6.8" data-path="linear-regression.html"><a href="linear-regression.html#lm-model-interp"><i class="fa fa-check"></i><b>6.8</b> Feature Interpretation</a></li>
<li class="chapter" data-level="6.9" data-path="linear-regression.html"><a href="linear-regression.html#final-thoughts"><i class="fa fa-check"></i><b>6.9</b> Final thoughts</a></li>
<li class="chapter" data-level="6.10" data-path="linear-regression.html"><a href="linear-regression.html#learning-more"><i class="fa fa-check"></i><b>6.10</b> Learning more</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>7</b> Logistic regression</a><ul>
<li class="chapter" data-level="7.1" data-path="logistic-regression.html"><a href="logistic-regression.html#prerequisites-4"><i class="fa fa-check"></i><b>7.1</b> Prerequisites</a></li>
<li class="chapter" data-level="7.2" data-path="logistic-regression.html"><a href="logistic-regression.html#why-logistic-regression"><i class="fa fa-check"></i><b>7.2</b> Why logistic regression</a></li>
<li class="chapter" data-level="7.3" data-path="logistic-regression.html"><a href="logistic-regression.html#simple-logistic-regression"><i class="fa fa-check"></i><b>7.3</b> Simple logistic regression</a></li>
<li class="chapter" data-level="7.4" data-path="logistic-regression.html"><a href="logistic-regression.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>7.4</b> Multiple logistic regression</a></li>
<li class="chapter" data-level="7.5" data-path="logistic-regression.html"><a href="logistic-regression.html#assessing-model-accuracy-1"><i class="fa fa-check"></i><b>7.5</b> Assessing model accuracy</a></li>
<li class="chapter" data-level="7.6" data-path="logistic-regression.html"><a href="logistic-regression.html#feature-interpretation"><i class="fa fa-check"></i><b>7.6</b> Feature interpretation</a></li>
<li class="chapter" data-level="7.7" data-path="logistic-regression.html"><a href="logistic-regression.html#final-thoughts-1"><i class="fa fa-check"></i><b>7.7</b> Final thoughts</a></li>
<li class="chapter" data-level="7.8" data-path="logistic-regression.html"><a href="logistic-regression.html#learning-more-1"><i class="fa fa-check"></i><b>7.8</b> Learning more</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regularized-regression.html"><a href="regularized-regression.html"><i class="fa fa-check"></i><b>8</b> Regularized regression</a><ul>
<li class="chapter" data-level="8.1" data-path="regularized-regression.html"><a href="regularized-regression.html#prerequisites-5"><i class="fa fa-check"></i><b>8.1</b> Prerequisites</a></li>
<li class="chapter" data-level="8.2" data-path="regularized-regression.html"><a href="regularized-regression.html#why"><i class="fa fa-check"></i><b>8.2</b> Why Regularize</a><ul>
<li class="chapter" data-level="8.2.1" data-path="regularized-regression.html"><a href="regularized-regression.html#ridge"><i class="fa fa-check"></i><b>8.2.1</b> Ridge penalty</a></li>
<li class="chapter" data-level="8.2.2" data-path="regularized-regression.html"><a href="regularized-regression.html#lasso"><i class="fa fa-check"></i><b>8.2.2</b> Lasso penalty</a></li>
<li class="chapter" data-level="8.2.3" data-path="regularized-regression.html"><a href="regularized-regression.html#elastic"><i class="fa fa-check"></i><b>8.2.3</b> Elastic nets</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="regularized-regression.html"><a href="regularized-regression.html#implementation"><i class="fa fa-check"></i><b>8.3</b> Implementation</a></li>
<li class="chapter" data-level="8.4" data-path="regularized-regression.html"><a href="regularized-regression.html#regression-glmnet-tune"><i class="fa fa-check"></i><b>8.4</b> Tuning</a></li>
<li class="chapter" data-level="8.5" data-path="regularized-regression.html"><a href="regularized-regression.html#lm-features"><i class="fa fa-check"></i><b>8.5</b> Feature interpretation</a></li>
<li class="chapter" data-level="8.6" data-path="regularized-regression.html"><a href="regularized-regression.html#attrition-data"><i class="fa fa-check"></i><b>8.6</b> Attrition data</a></li>
<li class="chapter" data-level="8.7" data-path="regularized-regression.html"><a href="regularized-regression.html#final-thoughts-2"><i class="fa fa-check"></i><b>8.7</b> Final thoughts</a></li>
<li class="chapter" data-level="8.8" data-path="regularized-regression.html"><a href="regularized-regression.html#learning-more-2"><i class="fa fa-check"></i><b>8.8</b> Learning more</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="MARS.html"><a href="MARS.html"><i class="fa fa-check"></i><b>9</b> Multivariate Adaptive Regression Splines</a><ul>
<li class="chapter" data-level="9.1" data-path="MARS.html"><a href="MARS.html#prerequisites-6"><i class="fa fa-check"></i><b>9.1</b> Prerequisites</a></li>
<li class="chapter" data-level="9.2" data-path="MARS.html"><a href="MARS.html#the-basic-idea"><i class="fa fa-check"></i><b>9.2</b> The basic idea</a><ul>
<li class="chapter" data-level="9.2.1" data-path="MARS.html"><a href="MARS.html#multivariate-regression-splines"><i class="fa fa-check"></i><b>9.2.1</b> Multivariate regression splines</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="MARS.html"><a href="MARS.html#fitting-a-basic-mars-model"><i class="fa fa-check"></i><b>9.3</b> Fitting a basic MARS model</a></li>
<li class="chapter" data-level="9.4" data-path="MARS.html"><a href="MARS.html#tuning"><i class="fa fa-check"></i><b>9.4</b> Tuning</a></li>
<li class="chapter" data-level="9.5" data-path="MARS.html"><a href="MARS.html#feature-interpretation-1"><i class="fa fa-check"></i><b>9.5</b> Feature interpretation</a></li>
<li class="chapter" data-level="9.6" data-path="MARS.html"><a href="MARS.html#attrition-data-1"><i class="fa fa-check"></i><b>9.6</b> Attrition data</a></li>
<li class="chapter" data-level="9.7" data-path="MARS.html"><a href="MARS.html#final-thoughts-3"><i class="fa fa-check"></i><b>9.7</b> Final thoughts</a></li>
<li class="chapter" data-level="9.8" data-path="MARS.html"><a href="MARS.html#learning-more-3"><i class="fa fa-check"></i><b>9.8</b> Learning more</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="RF.html"><a href="RF.html"><i class="fa fa-check"></i><b>10</b> Random Forests</a><ul>
<li class="chapter" data-level="10.1" data-path="RF.html"><a href="RF.html#prerequisites-7"><i class="fa fa-check"></i><b>10.1</b> Prerequisites</a></li>
<li class="chapter" data-level="10.2" data-path="RF.html"><a href="RF.html#decision-trees"><i class="fa fa-check"></i><b>10.2</b> Decision trees</a><ul>
<li class="chapter" data-level="10.2.1" data-path="RF.html"><a href="RF.html#a-simple-regression-tree-example"><i class="fa fa-check"></i><b>10.2.1</b> A simple regression tree example</a></li>
<li class="chapter" data-level="10.2.2" data-path="RF.html"><a href="RF.html#deciding-on-splits"><i class="fa fa-check"></i><b>10.2.2</b> Deciding on splits</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="RF.html"><a href="RF.html#bagging"><i class="fa fa-check"></i><b>10.3</b> Bagging</a></li>
<li class="chapter" data-level="10.4" data-path="RF.html"><a href="RF.html#random-forests"><i class="fa fa-check"></i><b>10.4</b> Random forests</a><ul>
<li class="chapter" data-level="10.4.1" data-path="RF.html"><a href="RF.html#oob-error-vs.test-set-error"><i class="fa fa-check"></i><b>10.4.1</b> OOB error vs.¬†test set error</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="RF.html"><a href="RF.html#fitting-a-basic-random-forest-model"><i class="fa fa-check"></i><b>10.5</b> Fitting a basic random forest model</a></li>
<li class="chapter" data-level="10.6" data-path="RF.html"><a href="RF.html#tuning-1"><i class="fa fa-check"></i><b>10.6</b> Tuning</a><ul>
<li class="chapter" data-level="10.6.1" data-path="RF.html"><a href="RF.html#tuning-via-ranger"><i class="fa fa-check"></i><b>10.6.1</b> Tuning via ranger</a></li>
<li class="chapter" data-level="10.6.2" data-path="RF.html"><a href="RF.html#tuning-via-caret"><i class="fa fa-check"></i><b>10.6.2</b> Tuning via caret</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="RF.html"><a href="RF.html#feature-interpretation-2"><i class="fa fa-check"></i><b>10.7</b> Feature interpretation</a></li>
<li class="chapter" data-level="10.8" data-path="RF.html"><a href="RF.html#attrition-data-2"><i class="fa fa-check"></i><b>10.8</b> Attrition data</a></li>
<li class="chapter" data-level="10.9" data-path="RF.html"><a href="RF.html#final-thoughts-4"><i class="fa fa-check"></i><b>10.9</b> Final thoughts</a></li>
<li class="chapter" data-level="10.10" data-path="RF.html"><a href="RF.html#learning-more-4"><i class="fa fa-check"></i><b>10.10</b> Learning more</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="11" data-path="appendix-data.html"><a href="appendix-data.html"><i class="fa fa-check"></i><b>11</b> (APPENDIX) Appendix {-}</a><ul>
<li class="chapter" data-level="" data-path="appendix-data.html"><a href="appendix-data.html#ames-iowa-housing-data-1"><i class="fa fa-check"></i>Ames Iowa housing data</a></li>
<li class="chapter" data-level="" data-path="appendix-data.html"><a href="appendix-data.html#employee-attrition-data-1"><i class="fa fa-check"></i>Employee attrition data</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Business Analytics with R: Descriptive, Predictive, and Prescriptive Methods</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="descriptive" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Descriptive Statistics</h1>
<p>The first step in any data analysis problem is to describe the data using descriptive statistics and look at the data using appropriate graphical techniques. In this chapter, we will introduce some basic descriptive statistics (e.g., various measures of location and spread) while graphical techniques are discussed in Chapter <a href="visualization.html#visualization">3</a>.</p>
<p>Descriptive statistics, in contrast to inferential statistics (Chapter <a href="inference.html#inference">4</a>), aim to describe a <em>data sample</em>, or <em>sample</em>. A sample is simply a set of data collected from some population of interest (e.g., the annual salaries of males at a particular company). In this book, we refer to the individual sample points as observations. Typically, the data are collected in a way such that the sample is representative of the population from which it came. Other times, we may have access to the entire population, in which case, our sample comprises a census. In either case, descriptive statistics seek to paint a quantitative picture of the data using simple values such as measures of <em>location</em> (i.e., what a typical values looks like) and <em>dispersion</em> (i.e., how spread out the data are). Beyond measures of location and spread, we also discuss percentiles and simple ways for detecting <em>outliers</em> (i.e., unusually small or large observations). Various other descriptive statistics and useful R packages are introduced in the exercises at the end of the chapter.</p>
<div id="prerequisites" class="section level2">
<h2><span class="header-section-number">2.1</span> Prerequisites</h2>
<p>The R package <code>stats</code> (which is part of the standard R distribution) provides functions for many of the descriptive statistics discussed in this chapter, plus more. For a complete list of functions, type <code>library(help = &quot;stats&quot;)</code> into the R console.</p>
<p>For illustration, we will use two data sets: (1) the Ames housing data set <span class="citation">(Cock <a href="#ref-ames-cock-2011">2011</a>)</span> (in particular, the column labelled <code>Sale_Price</code>) and (2) the Adult data set <span class="citation">(Lichman <a href="#ref-uci">2013</a>)</span>; both of which are described in Chapter <a href="intro.html#intro">1</a>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Ames housing data</span>
<span class="kw">dim</span>(ames &lt;-<span class="st"> </span>AmesHousing<span class="op">::</span><span class="kw">make_ames</span>())  <span class="co"># construct data and print dimensions</span>
## [1] 2930   81
<span class="kw">head</span>(Sale_Price &lt;-<span class="st"> </span>ames<span class="op">$</span>Sale_Price)  <span class="co"># extract Sale_Price column</span>
## [1] 215000 105000 172000 244000 189900 195500

<span class="co"># Adult data</span>
<span class="kw">data</span>(AdultUCI, <span class="dt">package =</span> <span class="st">&quot;arules&quot;</span>)  <span class="co"># load data from arules package</span>
<span class="kw">dim</span>(AdultUCI)  <span class="co"># print dimensions</span>
## [1] 48842    15</code></pre>
</div>
<div id="measures-of-location" class="section level2">
<h2><span class="header-section-number">2.2</span> Measures of location</h2>
<p>The first question we might ask of the <code>ames</code> data is ‚ÄúWhat is a typical value for the selling price?‚Äù In particular, we are interested in some measure of <em>central location</em> or <em>central tendency</em>. Such measures try to summarize a set of values with a ‚Äútypical‚Äù number. There are a number of different measures of location, but the simplest and most commonly used is the <em>arithmetic mean</em> or <em>sample mean</em>.</p>
<div id="the-sample-mean" class="section level3">
<h3><span class="header-section-number">2.2.1</span> The sample mean</h3>
<p>Suppose we have a set of <span class="math inline">\(n\)</span> observations denoted <span class="math inline">\(x_1, x_2, \dots, x_n\)</span>. The sample mean, denoted <span class="math inline">\(\bar{x}\)</span>, is defined as the sum of the observations divided <span class="math inline">\(n\)</span>:
<span class="math display">\[\begin{equation}
\label{eqn:sample-mean}
  \bar{x} = \frac{1}{n}\sum_{i = 1}^n x_i = \frac{1}{n}\left(x_1 + x_2 + \dots + x_n\right),
\end{equation}\]</span>
where <span class="math inline">\(\sum\)</span> is mathematical notation for summation and simply means ‚Äúadd up the values‚Äù. Another way to think of the sample mean is as the ``center of gravity‚Äô‚Äô for a set of observations. That is, if the observations were placed on a number line, like a teeter-totter, the sample mean would be the balancing point. For example, the sample mean of 1.7, 3.3, 7.5, 8.1, and 8.9 is <span class="math inline">\(\bar{x} = 5.9\)</span> which is displayed in Figure <a href="descriptive.html#fig:teeter-totter">2.1</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:teeter-totter"></span>
<img src="abar_files/figure-html/teeter-totter-1.png" alt="The sample mean as the balancing point of a set of five observations." width="480" />
<p class="caption">
Figure 2.1: The sample mean as the balancing point of a set of five observations.
</p>
</div>
<p>In R, we can obtain the sample mean of a set of observations using the <code>mean()</code> function</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(<span class="kw">c</span>(<span class="fl">1.7</span>, <span class="fl">3.3</span>, <span class="fl">7.5</span>, <span class="fl">8.1</span>, <span class="fl">8.9</span>))
## [1] 5.9</code></pre>
<p>For example, the sample mean of <code>Sale_Price</code> can be obtained using</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(Sale_Price)
## [1] 180796.1
<span class="kw">mean</span>(Sale_Price)  <span class="co"># alternatively</span>
## [1] 180796.1</code></pre>
<p>Thus, a typical or central value of selling price for the <code>ames</code> data frame would be $180,796.</p>

<div class="note">
Most of the descriptive statistical functions in R include an <code>na.rm</code> argument which defaults to <code>FALSE</code>. If <code>na.rm = FALSE</code>, the return value for these functions will be <code>NA</code> (i.e., R‚Äôs representation of a missing value; see <code>?NA</code> for details.) whenever the sample contains at least one <code>NA</code>. If set to <code>TRUE</code>, then all <code>NA</code>s will be removed from the sample prior to computing the statistic. This is illustrated in the following code chunk:
</div>

<pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">18</span>, <span class="dv">4</span>, <span class="dv">12</span>, <span class="ot">NA</span>, <span class="dv">19</span>)
<span class="kw">mean</span>(x)
## [1] NA
<span class="kw">mean</span>(x, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)
## [1] 13.25
(<span class="dv">18</span> <span class="op">+</span><span class="st"> </span><span class="dv">4</span> <span class="op">+</span><span class="st"> </span><span class="dv">12</span> <span class="op">+</span><span class="st"> </span><span class="dv">19</span>) <span class="op">/</span><span class="st"> </span><span class="dv">4</span>  <span class="co"># sanity check</span>
## [1] 13.25</code></pre>
</div>
<div id="the-sample-median" class="section level3">
<h3><span class="header-section-number">2.2.2</span> The sample median</h3>
<p>One problem with the sample mean is that it is not <em>robust</em> to outliers. To illustrate, suppose we have a sample of size two: <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> so that <span class="math inline">\(\bar{x} = \left(x_1 + x_2\right) / 2\)</span>. Then, regardless of the value of <span class="math inline">\(x_1\)</span>, we can change <span class="math inline">\(x_2\)</span> to achieve any value for the sample mean. Since it only takes one of the <span class="math inline">\(n\)</span> observations to arbitrarily change <span class="math inline">\(\bar{x}\)</span>, we say that <span class="math inline">\(\bar{x}\)</span> has a finite sample breakdown point (FSBP) of <span class="math inline">\(1 / n\)</span>. The higher the FSBP of a sample statistic, the less affected it is to outliers (i.e., the more robust it is). The highest FSBP a sample statistic can obtain is 50%.</p>
<p>A more robust measure of location is given by the <em>sample median</em>. Consider a sample of size <span class="math inline">\(n\)</span>: <span class="math inline">\(x_1, x_2, \dots, x_n\)</span>. Let <span class="math inline">\(x_{\left(i\right)}\)</span> denote the <span class="math inline">\(i\)</span>-th observation after the sample has been sorted in ascending order. The sample median, denoted <span class="math inline">\(M\)</span>, is defined as
<span class="math display">\[\begin{equation*}
  M = 
  \begin{cases}
    x_{\left(m\right)} &amp; \quad \text{if } n \text{ is odd}\\
    \left(x_{\left(m\right)} + x_{\left(m + 1\right)}\right) / 2 &amp; \quad \text{if } n \text{ is even},
  \end{cases}
\end{equation*}\]</span>
where <span class="math inline">\(m = \left(n + 1\right) / 2\)</span>. In other words, if <span class="math inline">\(n\)</span> is odd, the sample median is just the middle number, otherwise, we take the sample mean of the two middle numbers.</p>
<p>Since the sample median only depends on the middle number (or middle two numbers), it is far more robust than the sample mean. In fact, the sample median has an FSBP of roughly 50%. In other words, close to 50% of the observations have to be outliers in order to affect the sample median. This makes the sample median more useful in practice when dealing with data that contain outliers or come from skewed distributions.</p>
<p>In R, we can compute the sample median using the <code>median()</code> function. For the Ames housing data, the median sale price is $160,000, which can be computed using</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">median</span>(Sale_Price)
## [1] 160000
<span class="kw">median</span>(ames<span class="op">$</span>Sale_Price)  <span class="co"># alternatively</span>
## [1] 160000</code></pre>
</div>
<div id="the-mean-or-the-median" class="section level3">
<h3><span class="header-section-number">2.2.3</span> The mean or the median</h3>
<p>So which should be used in practice, the sample mean or the sample median? If the data are roughly <em>symmetric</em>, then the sample mean and sample median will be approximately equal. In fact, when the sample mean is most useful for reporting, it will typically be close to the sample median. If, however, the data are skewed to the left or right, then the sample mean will tend to get pulled in the same direction. For example, for right (or positively) skewed data, the sample mean will typically be larger than the sample median (sometimes much larger). In these cases, the sample median is a more reliable measure of location. However, there is nothing wrong with reporting both statistics.</p>
<p>For the <code>ames</code> data frame, the sample median was smaller than the sample mean by $20,796.06. This is not surprising since data on housing and sale prices tend to be skewed right which can inflate the sample mean. In this case, the sample median will be a better measure of location than the sample mean. Different approaches to detecting skewness will be discussed in Chapter <a href="visualization.html#visualization">3</a> when we talk about visualizing data.</p>
</div>
</div>
<div id="measures-of-spread" class="section level2">
<h2><span class="header-section-number">2.3</span> Measures of spread</h2>
<p>Measures of location by themselves are not very useful. We often want to know how ``spread out‚Äô‚Äô the data are. This can be summarized using various measures of <em>spread</em> or <em>dispersion</em>.</p>
<p>The most common measure of spread is the <em>sample variance</em>, denoted by <span class="math inline">\(s^2\)</span>. The sample variance of a sample is defined as
<span class="math display">\[\begin{equation*}
  s ^ 2 = \sum_{i = 1} ^ n \left(x_i - \bar{x}\right) ^ 2 / \left(n - 1\right).
\end{equation*}\]</span>
That is, the sample variance is just the sum of the squared deviations of each observation from the sample mean dived by <span class="math inline">\(n - 1\)</span>. The <span class="math inline">\(n - 1\)</span> is used to make <span class="math inline">\(s ^ 2\)</span> an <em>unbiased estimator</em><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> of the population variance. Since the sample variance involves squaring the differences, it does not retain the original units, unlike the sample mean and variance. Oftentimes the positive square root of the sample variance, called the <em>sample standard deviation</em>, is used instead. The sample standard deviation, denoted <span class="math inline">\(s\)</span>, is more useful because is has the same units as the original observations (e.g., feet, dollars, etc.).</p>
<p>In R, the sample variance and sample standard deviation can be computed using the functions <code>var()</code> and <code>sd()</code>, respectively. The following example illustrates their use on the <code>ames</code> data frame using the variable <code>Sale_Price</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(Sale_Price)  <span class="co"># sample variance</span>
## [1] 6381883616
<span class="kw">sd</span>(Sale_Price)  <span class="co"># sample standard deviation</span>
## [1] 79886.69
<span class="kw">sqrt</span>(<span class="kw">var</span>(Sale_Price))  <span class="co"># sanity check</span>
## [1] 79886.69</code></pre>
<p>Since the sample standard deviation retains the original units, we can report this in dollar amount (e.g., the standard deviation of sale prices for homes sold from 2006‚Äì2010 is $79,886.69).</p>
<div id="empirical-rule" class="section level3">
<h3><span class="header-section-number">2.3.1</span> The empirical rule</h3>
<p>It is possible for the distribution of some of the variables in a data sets to exhibit a ‚Äúbell shape‚Äù üîî. For bell-shaped distributions, the <em>empirical rule</em>, also known as the <em>68-95-99.7 rule</em>, states that (roughly) 68% of the observations should fall within one standard deviation of the mean, 95% should fall within two standard deviations of the mean, and 99.7% should fall within three standard deviations of the mean‚Äîthis is illustrated in Figure <a href="descriptive.html#fig:empiricial-rule">2.2</a>. Therefore, data from bell-shaped distributions can be adequately described my the sample mean and and standard deviation (if it exists). The most important takeaway is that the majority of observations from a bell-shaped distribution should be within a couple of standard deviations from the mean, and it is extremely unlikely for an observation to be beyond three standard deviations from the mean. This provides an intuitive and simple rule for identifying potential outliers (see Section <a href="descriptive.html#outliers">2.6</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:empiricial-rule"></span>
<img src="abar_files/figure-html/empiricial-rule-1.png" alt="The empricial rule for bell-shaped distributions. (In progress!)" width="672" />
<p class="caption">
Figure 2.2: The empricial rule for bell-shaped distributions. (In progress!)
</p>
</div>
<p>To reiterate, the empirical rule applies to data that are (at least approximately) bell-shaped. To ascertain the shape of the distribution of a sample, a <em>histogram</em> or <em>kernel density estimate</em> can be used‚Äîthese are discussed in Chapter <a href="visualization.html#visualization">3</a>. A histogram of the <code>Sale_Price</code> data is displayed in the left side of Figure <a href="descriptive.html#fig:sale-price-hist">2.3</a>. These data are not bell-shaped, or even symmetric‚Äîin fact, <code>Sale_Price</code> appears to be skew right. Data that are skew right can often be transformed to appear more bell-shaped by taking a logarithm or square root transformation. A histogram of <code>log(Sale_Price)</code> is displayed in the right side of Figure <a href="descriptive.html#fig:sale-price-hist">2.3</a>. From the histograms, it is clear that <code>Sale_Price</code> is in fact skew right and that taking the logarithm makes the distribution appear more bell-shaped‚Äîsuch transformations are useful for some of the statistical inference procedures discussed in Chapter <a href="inference.html#inference">4</a> which assume that the data are approximately normally distributed. <strong>FIXME: What other methods in this book assume normality (and therefore symmetry)? For example, regression.</strong></p>
<div class="figure" style="text-align: center"><span id="fig:sale-price-hist"></span>
<img src="abar_files/figure-html/sale-price-hist-1.png" alt="Histogram estimates of the distribution of `Sale_Price` (left) and `log(Sale_Price)` (right)." width="80%" />
<p class="caption">
Figure 2.3: Histogram estimates of the distribution of <code>Sale_Price</code> (left) and <code>log(Sale_Price)</code> (right).
</p>
</div>
</div>
</div>
<div id="percentiles" class="section level2">
<h2><span class="header-section-number">2.4</span> Percentiles</h2>
<p>The <span class="math inline">\(p\)</span>-th percentile of a sample, denoted <span class="math inline">\(x_p\)</span>, is the value for which <span class="math inline">\(p\)</span> percent of the observations are less than <span class="math inline">\(x_p\)</span>. The median, for example, is the 50-th percentile (i.e., the middle number). Names are given to special groups of percentiles. <em>Deciles</em>, for example, divide a sample into ten equally sized buckets. <em>Quartiles</em> are the values that divide the data into four equally sized groups‚Äîin other words, the quartiles of a sample consists of the 25-th, 50-th, and 75-th percentiles. The quartiles, denoted <span class="math inline">\(Q_1\)</span>, <span class="math inline">\(Q_2\)</span>, and <span class="math inline">\(Q_3\)</span> (<span class="math inline">\(Q_1\)</span> and <span class="math inline">\(Q_3\)</span> are also referred to as the lower and upper quartiles, respectively), play an important part in many descriptive and graphical analyses. Together with measures of location and spread, the percentiles help describe the shape of the sample (i.e., what its distribution looks like). In fact, one of the most useful graphics for describing a sample is the <em>boxplot</em> which is described in Chapter <a href="visualization.html#visualization">3</a>. The boxplot is a simple visualization capturing the quartiles, median, as well as the maximum and minimum values and is extremely effective at showing the shape of a set of data (these five summary statistics are collectively known as Tukey‚Äôs <em>five-number summary</em>). A modern alternative to boxplots, called <em>violin plots</em>, will also be discussed in Chapter <a href="visualization.html#visualization">3</a>.</p>
<p>The formula for computing the <span class="math inline">\(p\)</span>-th percentile for a sample is not unique and many definitions exist. In fact, R includes nine different algorithms (controlled via the <code>type</code> argument) for computing percentiles! Therefore, it is important to realize that different software may produce slightly different results when computing percentiles.</p>

<div class="note">
To reproduce the same quantiles provided by SAS<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>, specify <code>type = 3</code> in the call to <code>quantile()</code>. <strong>FIXME: This needs to be verified!</strong>
</div>

<p>The R function for computing percentiles is <code>quantile()</code>. Quantiles are essentially the same as percentiles, but specified using decimals rather than percentages. For example, the 5-th percentile is equivalent to the 0.05 quantile. The following code chunk computes the quartiles of <code>Sale_Price</code> from the <code>ames</code> data frame. We use the default algorithm (i.e., <code>type = 7</code>); for specifics, see the help page <code>?stats::quantile</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">quantile</span>(Sale_Price, <span class="dt">probs =</span> <span class="kw">c</span>(<span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>))
##    25%    50%    75% 
## 129500 160000 213500</code></pre>
<p>In other words, 25% of the sale prices were below $129,500, 25% between $129,500 and $160,000, 25% between $160,000 and $213,500, and the rest greater than $213,500.</p>
</div>
<div id="robust-measures-of-spread" class="section level2">
<h2><span class="header-section-number">2.5</span> Robust measures of spread</h2>
<p>Since the sample standard deviation relies on squared deviations from the sample mean (a non-robust measure of location), it is also sensitive to outliers. A measure of spread less affected by outliers is the <em>interquartile range</em> (IQR). The IQR is defined as the difference between the upper and lower quartiles:
<span class="math display">\[\begin{equation*}
  \text{IQR} = Q_3 - Q_1.
\end{equation*}\]</span>
The IQR describes the variability of the middle 50% of the data and has an FSBP of 25%. Therefore, the IQR is a more robust measure of spread than the sample standard deviation.</p>
<p>Perhaps a more useful, but less often used, robust measure of spread is the <em>median absolute deviation</em> (MAD). For a sample <span class="math inline">\(x_1, x_2, \dots, x_n\)</span> with sample median <span class="math inline">\(M\)</span>, the MAD is given by the median of the absolute value of the deviations from <span class="math inline">\(M\)</span>:
<span class="math display">\[\begin{equation*}
  \text{MAD} = median\left(\left|x_1 - M\right|, \left|x_2 - M\right|, \dots, \left|x_n - M\right|\right).
\end{equation*}\]</span>
The MAD, like the median, has an FSBP of 50%, meaning nearly half the observations could be outliers without affecting the MAD. Some software, including R by default, actually computes an adjusted version of MAD, called MADN, by multiplying by the constant <span class="math inline">\(1.4826\)</span>: <span class="math inline">\(\text{MADN} = 1.4826 \times \text{MAD}\)</span>. This adjustment is to make MAD <em>asymptotically normally consistent</em> for the population standard deviation <span class="math inline">\(\sigma\)</span>. In other words, as the sample size <span class="math inline">\(n\)</span> gets larger, MADN is a good estimator of <span class="math inline">\(\sigma\)</span> for a normally distributed population. In practice, MADN is typically used/reported.</p>
<p>To compute the IQR or MAD(N) in R, we can use the <code>IQR()</code> and <code>mad()</code> functions, respectively. For the <code>ames</code> data frame, the IQR and MAD(N) for <code>Sale_Price</code> are computed below. Note that since the IQR is based on the 25-th and 75-th percentiles, the <code>IQR()</code> function also includes the option <code>type</code> for specifying which algorithm to use for computing <span class="math inline">\(Q_1\)</span> and <span class="math inline">\(Q_3\)</span>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">IQR</span>(Sale_Price)
## [1] 84000
<span class="kw">mad</span>(Sale_Price)  <span class="co"># MADN</span>
## [1] 54856.2
<span class="kw">mad</span>(Sale_Price, <span class="dt">constant =</span> <span class="dv">1</span>)  <span class="co"># MAD</span>
## [1] 37000
<span class="kw">mad</span>(Sale_Price, <span class="dt">constant =</span> <span class="dv">1</span>) <span class="op">*</span><span class="st"> </span><span class="fl">1.4826</span>  <span class="co"># sanity check</span>
## [1] 54856.2</code></pre>
<p>In practice, it is common (and important) to report some measure of spread whenever reporting measures of location (and vice versa). The standard deviation is often reported with the mean. Whenever the median is used, the IQR or MAD(N) is often reported as well.</p>
</div>
<div id="outliers" class="section level2">
<h2><span class="header-section-number">2.6</span> Outlier detection</h2>
<p>In this section, we present a few simple rules for detecting potential outliers in univariate data; that is, data on a single variable. In later chapters, we present more sophisticated methods for detecting potential outliers and anomalies in multivariate data (i.e., data on more than one variable).</p>
<p>The empirical rule (Section <a href="descriptive.html#empirical-rule">2.3.1</a>) probably offers the simplest method for detecting outliers, at least for reasonably bell-shaped data. Recall that for approximately bell-shaped distributions, 95% of the observations should lie within two standard deviations of the mean. Therefore, for approximately bell-shaped distributions, <span class="math inline">\(z\)</span>-scores greater than two in absolute value might be considered unusual (a cutoff of 2.24 has been shown to be more useful in practice <span class="citation">(Wilcox <a href="#ref-wilcox-applying-2003">2003</a>)</span>). To make this rule more robust, we can compute a modified <span class="math inline">\(z\)</span>-score based on the median and MADN. Below, we define a simple function for detecting outliers according to the empirical rule:</p>
<pre class="sourceCode r"><code class="sourceCode r">detect_outliers &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">robust =</span> <span class="ot">FALSE</span>, <span class="dt">cutoff =</span> <span class="dv">2</span>) {
  z_score &lt;-<span class="st"> </span><span class="cf">if</span> (robust) {
    (x <span class="op">-</span><span class="st"> </span><span class="kw">median</span>(x)) <span class="op">/</span><span class="st"> </span><span class="kw">mad</span>(x)  <span class="co"># modified z-score</span>
  } <span class="cf">else</span> {
    (x <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(x)) <span class="op">/</span><span class="st"> </span><span class="kw">sd</span>(x)  <span class="co"># z-score</span>
  }
  <span class="kw">sort</span>(x[<span class="kw">abs</span>(z_score) <span class="op">&gt;</span><span class="st"> </span>cutoff])
}</code></pre>
<p>Next, we simulate some data from a standard normal distribution (i.e., a bell-shaped distribution with mean zero and standard deviation one) with two outliers (5 and -100) and test out our <code>detect_outliers()</code> function. For a standard normal distribution, we would expect any observations greater than two in absolute value to be unlikely and should be flagged as potential outliers.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">101</span>)  <span class="co"># for reproducibility</span>
x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">rnorm</span>(<span class="dv">100</span>), <span class="dv">5</span>, <span class="dv">-100</span>)
<span class="kw">detect_outliers</span>(x)
## [1] -100
<span class="kw">detect_outliers</span>(x, <span class="dt">robust =</span> <span class="ot">TRUE</span>)
## [1] -100.000000   -2.319327   -2.073106   -2.050308    5.000000
<span class="kw">detect_outliers</span>(x, <span class="dt">cutoff =</span> <span class="fl">2.24</span>)  <span class="co"># following Wilcox (2003)</span>
## [1] -100</code></pre>
<p>Notice that five does not get flagged as an outlier when using the standard <span class="math inline">\(z\)</span>-score based on the sample mean and standard deviation. This is due to the fact that the extreme value -100 skews these descriptive statistics. Using the robust method, however, returns reasonable results.</p>
<p>Using the empirical rule is rather limited in practice since distributions are often not bell-shaped, or even symmetric‚Äîthough, as seen with <code>Sale_Price</code>, some can be transformed to appear more bell-shaped. A better outlier detection rule can be constructed using the IQR. This is the same method used to flag outliers in boxplots (see Section <a href="visualization.html#boxplots">3.2.1.3</a>). The general rule is to define an observation an outlier if it lies outside the interval <span class="math inline">\(\left(Q_1 - 1.5 \times IQR, Q_2 + 1.5 \times IQR\right)\)</span>. While it would be simple to write our own function for detecting outliers using the boxplot method, we can use R‚Äôs built-in function <code>boxplot.stats()</code> (see <code>?grDevices::boxplot.stats</code> for details). Notice how this method happens to catch the true outliers!</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">boxplot.stats</span>(x)<span class="op">$</span>out
## [1]    5 -100</code></pre>
<p>We leave it to Exercise <a href="#ames-outlier"><strong>??</strong></a> to further explore outliers for the <code>Sale_Price</code> variable in the <code>ames</code> housing data frame.</p>
</div>
<div id="categorical" class="section level2">
<h2><span class="header-section-number">2.7</span> Describing categorical data</h2>
<p>All of the descriptive statistics discussed thus far are appropriate for <em>continuous variables</em><a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>‚Äîthat is, variables that can be measured on a continuum (e.g., the weight of an object measured in grams). Technically, no variable can be truly measured on a continuous scale due to precision limitations with how all variables are measured. So, in a sense, continuous variables in practice are numeric variables that can take on a lot of values. Oftentimes the data we are describing contains <em>categorical variables</em>. A categorical variable is a variable whose measurement scale consists of a set of categories (e.g., manufacturer and gender). Such variables are easy to describe using <em>contingency</em> or <em>cross-classification tables</em> (e.g., tables of frequencies and proportions).</p>
<p>Categorical variables fall into one of two types: <em>nominal</em> and <em>ordinal</em>. Nominal variables are categorical variables whose categories do no have a natural ordering. The categories of an ordinal variable do have a natural ordering, but no defined distance between the categories. For example, the <code>AdultUCI</code> data frame contains the columns <code>income</code> (with unique categories <code>&quot;small&quot;</code> and <code>&quot;large&quot;</code>) and <code>sex</code> (with unique categories <code>&quot;Female&quot;</code> and <code>&quot;Male&quot;</code>). <code>income</code> would be an example of an ordered variable (since <code>&quot;small&quot; &lt; &quot;large&quot;</code>, but <code>&quot;large&quot; - &quot;small&quot;</code> has no meaningful interpretation) while <code>sex</code> is nominal.</p>
<p>In R, categorical variables are typically represented using the <code>&quot;factor&quot;</code> class, but can also be represented by character strings (i.e., the more general <code>&quot;character&quot;</code> class). The <code>AdultUCI</code> data set contains several factors:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">which</span>(<span class="kw">sapply</span>(AdultUCI, is.factor))  <span class="co"># positions and names of factor columns</span>
##      workclass      education marital-status     occupation   relationship 
##              2              4              6              7              8 
##           race            sex native-country         income 
##              9             10             14             15
<span class="kw">which</span>(<span class="kw">sapply</span>(AdultUCI, is.character))  <span class="co"># sanity check</span>
## named integer(0)
<span class="kw">which</span>(<span class="kw">sapply</span>(AdultUCI, is.ordered))  <span class="co"># check specifically for ordered factors</span>
## education    income 
##         4        15</code></pre>
<p>To coerce a variable into a nominal or ordinal factor, we can use the functions <code>as.factor()</code> and <code>as.ordered()</code>, respectively. Some of the techniques discussed in this book can take ordinality into account, so it is good practice to make sure such variables are coerced to ordered factors.</p>
<div id="contingency-tables" class="section level3">
<h3><span class="header-section-number">2.7.1</span> Contingency tables</h3>
<p>A <em>contingency table</em> is a rectangular table containing the frequencies (or proportions) of observations within the different categories of one or more categorical variables. Contingency tables typically cross-classifiy two categorical variables, but can be used to cross-classifiy more than two. R contains a number of functions for creating such tables, the most useful probably being <code>xtabs()</code> since it has a formula interface. To illustrate, we construct a <span class="math inline">\(2 \times 2\)</span> table cross-classifying <code>income</code> and <code>sex</code> from the <code>AdultUCI</code> data frame:</p>
<pre class="sourceCode r"><code class="sourceCode r">(tab &lt;-<span class="st"> </span><span class="kw">xtabs</span>(<span class="op">~</span><span class="st"> </span>sex <span class="op">+</span><span class="st"> </span>income, <span class="dt">data =</span> AdultUCI))
##         income
## sex      small large
##   Female  9592  1179
##   Male   15128  6662</code></pre>
<p>This table shows some disparity in income between females and males. In Chapter <a href="inference.html#inference">4</a>, we discuss ways of testing for association between a set of categorical variables.</p>
<p>We can also request that margins (i.e., row/column summaries) be added to the table using the <code>addmargins()</code> function:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">addmargins</span>(tab)  <span class="co"># defaults to adding row/column totals</span>
##         income
## sex      small large   Sum
##   Female  9592  1179 10771
##   Male   15128  6662 21790
##   Sum    24720  7841 32561
<span class="kw">addmargins</span>(tab, <span class="dt">FUN =</span> mean)  <span class="co"># add sample means to the margins instead</span>
## Margins computed over dimensions
## in the following order:
## 1: sex
## 2: income
##         income
## sex         small    large     mean
##   Female  9592.00  1179.00  5385.50
##   Male   15128.00  6662.00 10895.00
##   mean   12360.00  3920.50  8140.25</code></pre>
<p>We can can convert the frequencies to proportions using the <code>prop.table()</code> function:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">prop.table</span>(tab)
##         income
## sex           small      large
##   Female 0.29458555 0.03620896
##   Male   0.46460490 0.20460060</code></pre>
<p>In Section <a href="#cramers-v"><strong>??</strong></a>, we discuss a statistic that can be used to quantify the strength of the association between two categorical variables.</p>
</div>
</div>
<div id="exercises" class="section level2">
<h2><span class="header-section-number">2.8</span> Exercises</h2>

<div class="exercise">
<span id="exr:unnamed-chunk-3" class="exercise"><strong>Exercise 2.1  (Groupwise descriptive statistics)  </strong></span>For the <code>ames</code> data set, we computed various measures of spread and location for the variable <code>Sale_Price</code>. However, these data span multiple years (2006‚Äì2010). Therefore, it might be of interest to see how various descriptive statistics change over time. For this exercise, compute the sample median and MADN for <code>Sale_Price</code> stratified by the year in which the house was sold (i.e., <code>Year_Sold</code>). <strong>Hint:</strong> use the built-in functions <code>tapply()</code> or <code>by()</code>; see <code>?tapply</code> and <code>?by</code> for example usage. Do typical sale prices seem to be different across the five years?
</div>

<!-- Solution -->

<div class="exercise">
<span id="exr:ames-outlier" class="exercise"><strong>Exercise 2.2  (Outlier detection)  </strong></span>Use the boxplot outlier detection method on the variable <code>Sale_Price</code> from the <code>ames</code> housing example. How many outliers are detected using this method? How many outliers are detected using the empirical rule? Does the empirical rule seem appropriate for these data? Why or why not?
</div>

<!-- Solution -->

<div class="exercise">
<span id="exr:unnamed-chunk-6" class="exercise"><strong>Exercise 2.3  (Kurtosis and skewness)  </strong></span>TBD.
</div>


<div class="exercise">
<span id="exr:unnamed-chunk-7" class="exercise"><strong>Exercise 2.4  (The <code>apply()</code> function)  </strong></span>The <code>votes.repub</code> data set from the standard R package <code>cluster</code> <span class="citation">(Maechler et al. <a href="#ref-pkg-cluster">2016</a>)</span> contains the percentages of votes given to the republican candidate in presidential elections from 1856 to 1976. The rows represent the 50 US states, and the columns represent the 31 elections. The data can be loaded into R using <code>data(votes.repub, package = &quot;cluster&quot;)</code>. In this case, we may be interested in computing various descriptive statistics for each state across all years. Since the data are stored in rectangular format (i.e., state are in rows and years are in columns) we can efficiently compute descriptive statistics for each state using R‚Äôs built-in <code>apply()</code> function (see <code>?apply</code> for details). Using <code>apply()</code>, compute the sample mean percentage of votes and standard deviation for each state, taking care to handle <code>NA</code>‚Äôs appropriately. Which state had the highest average percentage of votes? Which had the lowest?
</div>


<div class="exercise">
<span id="exr:unnamed-chunk-8" class="exercise"><strong>Exercise 2.5  (Trimmed mean)  </strong></span>TBD.
</div>


<div class="exercise">
<span id="exr:unnamed-chunk-9" class="exercise"><strong>Exercise 2.6  (Sample variance <code>r emo::ji('scream')</code>)  </strong></span>Show that the formula for the sample is equivalent to
<span class="math display">\[
  s^2 = \left(\frac{1}{n} \sum_{i = 1}^n x_i^2\right) - \bar{x}^2.
\]</span>
</div>


</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-ames-cock-2011">
<p>Cock, Dean De. 2011. ‚ÄúAmes, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project.‚Äù <em>Journal of Statistics Education</em> 19 (3): 1‚Äì15. <a href="http://ww2.amstat.org/publications/jse/v19n3/decock.pdf" class="uri">http://ww2.amstat.org/publications/jse/v19n3/decock.pdf</a>.</p>
</div>
<div id="ref-uci">
<p>Lichman, M. 2013. ‚ÄúUCI Machine Learning Repository.‚Äù University of California, Irvine, School of Information; Computer Sciences. <a href="http://archive.ics.uci.edu/ml" class="uri">http://archive.ics.uci.edu/ml</a>.</p>
</div>
<div id="ref-wilcox-applying-2003">
<p>Wilcox, Rand R. 2003. <em>Applying Contemporary Statistical Techniques</em>. Academic Press.</p>
</div>
<div id="ref-pkg-cluster">
<p>Maechler, Martin, Peter Rousseeuw, Anja Struyf, Mia Hubert, and Kurt Hornik. 2016. <em>Cluster: Cluster Analysis Basics and Extensions</em>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>An unbiased estimator is one that does not, from sample to sample, systematically underestimate or overestimate a population parameter of interest. The sample mean is another example, as it is provides an unbiased estiamte of the population mean; that is, on average, the sample mean will be equal to the true population mean.<a href="descriptive.html#fnref1" class="footnote-back">‚Ü©</a></p></li>
<li id="fn2"><p>TBD.<a href="descriptive.html#fnref2" class="footnote-back">‚Ü©</a></p></li>
<li id="fn3"><p>Continuous variables, also referred to as quantitative variables, can be further categorized as <em>interval</em> or <em>ratio</em> variables. Though we do not make such distinction in this book, the interested reader is pointed to <strong>NEED REFERENCE</strong><a href="descriptive.html#fnref3" class="footnote-back">‚Ü©</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="visualization.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["abar.pdf", "abar.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
